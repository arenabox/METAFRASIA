{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Strandbox"
      ],
      "metadata": {
        "id": "0UfpNWch0Jkj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "5RknJ-e10Jqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before installing any packages, make sure to turn on Colab GPU as: Runtime > Change Runtime Type > T4 GPU"
      ],
      "metadata": {
        "id": "0dqH5zSJ0nee"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9oSdu5y0Cw4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/MaartenGr/BERTopic.git@master\n",
        "!pip install cudf-cu11 dask-cudf-cu11 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cuml-cu11 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cugraph-cu11 --extra-index-url=https://pypi.nvidia.com\n",
        "!pip install cupy-cuda11x -f https://pip.cupy.dev/aarch64\n",
        "!pip install safetensors\n",
        "!pip install datasets\n",
        "!pip install datashader\n",
        "!pip install adjustText"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "IySHyWuM0m1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert Text into Bert Chunks\n",
        "\n",
        "Most of the transformer based models accept token length of 512, therefore we convert text into small chunks of 512 tokens."
      ],
      "metadata": {
        "id": "frw3pGDK1RLK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "3exF4OVk1wzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "def create_chunks(all_text):\n",
        "  all_chunks = []\n",
        "  curr_chunk = ''\n",
        "  prev_len = 0\n",
        "  max_len = 512\n",
        "  for text in tqdm(all_text):\n",
        "    l = len(tokenizer(text)['input_ids'])\n",
        "    if l+prev_len < max_len:\n",
        "      curr_chunk += text\n",
        "      prev_len += l\n",
        "    else:\n",
        "      all_chunks.append(curr_chunk)\n",
        "      curr_chunk = text\n",
        "      prev_len = l\n",
        "  return all_chunks\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vxRxuaOP1tHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2', truncation=True,\n",
        "                                          padding=True,max_length=512,return_tensors='pt')"
      ],
      "metadata": {
        "id": "blBZapCn2XEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Processing\n",
        "\n",
        "Here we process the text extracted from scientific articles. Input file is in json format, which is prepared by the pipeline introduced in [ArenaBox](https://github.com/arenabox/ArenaBox). Here we follow example of json file of EIST journal.\n",
        "\n",
        "Sample format of json file looks as follows:\n",
        "\n",
        "\n",
        "```\n",
        "\"1\": {\n",
        "        \"file_name\": \"-It-s-not-talked-about---The-risk-of-failure-_2020_Environmental-Innovation-\",\n",
        "        \"doi\": \"10.1016/j.eist.2020.02.008\",\n",
        "        \"title\": \"\\u201cIt's not talked about\\u201d: The risk of failure in practice in sustainability experiments\",\n",
        "        \"abstract\": \"Scholars of sustainability transition have given much attention to local experiments in .....\",\n",
        "        \"text\": {\n",
        "            \"Introduction\":...,\n",
        "            ..\n",
        "            \"Conclusion\": ...,\n",
        "        },\n",
        "        \"location\": \"UK\",\n",
        "        \"raw_text\": \" A transition away from the use of fossil fuels ...\"\n",
        "        \"year\": \"2020\"\n",
        "    },\n",
        "    \"2\": {\n",
        "      ....\n",
        "    }\n",
        "\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "NS63HcHK2mIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "base_path = 'PATH/TO/EIST/FOLDER'\n",
        "with open(f'{base_path}/EIST_PDFS_TM.json', 'r') as f:\n",
        "  eist = json.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "bEkyAtfL2oFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2title = dict()\n",
        "\n",
        "for id, d in eist.items():\n",
        "  id2title[f'eist-{id}'] = d['title']"
      ],
      "metadata": {
        "id": "yEQ5jDJi3SPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fn, fd in eist.items():\n",
        "  raw_text = fd['raw_text'].split('.')\n",
        "  raw_text_chunks = create_chunks(raw_text)\n",
        "  eist[fn]['raw_text_chunks'] = raw_text_chunks"
      ],
      "metadata": {
        "id": "rrb_XmoP4V2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2id = dict()\n",
        "\n",
        "for fn, fd in eist.items():\n",
        "  for text in fd['raw_text_chunks']:\n",
        "    text2id[text] = f'eist-{fn}'"
      ],
      "metadata": {
        "id": "XNpzkgds4Y44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = list(text2id.keys())"
      ],
      "metadata": {
        "id": "zVWWSSSN4bA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we save a list of 512 tokens text in a json file\n",
        "with open(f'{base_path}/only_text.json', 'w+') as f:\n",
        "  json.dump({'text': data}, f, indent=4)"
      ],
      "metadata": {
        "id": "SmIPiAz75fMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation\n",
        "\n",
        "Here we convert chunked data into embeddings to be processed by topic models. We create multiple files which will be used by topic model. Instead of creating these files everytime we perform topic model, we will save it once and use it whenever we need to do topic modelling."
      ],
      "metadata": {
        "id": "JI7far5N5Oy1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "lY3dujV76ext"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "def get_embeddings(embedding_model, data):\n",
        "\n",
        "  embeddings = embedding_model.encode(data, show_progress_bar=True)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "Tfcgl4To6xDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create and Save Embeddings files"
      ],
      "metadata": {
        "id": "t8nQuHzd7i6f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Extract vocab to be used in BERTopic\n",
        "vocab = collections.Counter()\n",
        "tokenizer = CountVectorizer().build_tokenizer()\n",
        "for doc in tqdm(data):\n",
        "  vocab.update(tokenizer(doc))\n",
        "vocab = [word for word, frequency in vocab.items() if frequency >= 15]; len(vocab)\n",
        "\n",
        "with open(f'{base_path}/vocab.txt', 'wb') as fp:\n",
        "    pickle.dump(vocab, fp)"
      ],
      "metadata": {
        "id": "mmU7RGDn4dQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = get_embeddings(embedding_model, data)\n",
        "\n",
        "with open(f'{base_path}/embeddings.npy', 'wb') as f:\n",
        "    np.save(f, embeddings)"
      ],
      "metadata": {
        "id": "2pC-GgOC4gfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Train model and reduce dimensionality of embeddings\n",
        "umap_model = UMAP(n_components=5, n_neighbors=15, random_state=42, metric=\"cosine\", verbose=True)\n",
        "reduced_embeddings = umap_model.fit_transform(embeddings)\n",
        "\n",
        "with open(f'{base_path}/umap_embeddings.npy', 'wb') as f:\n",
        "    np.save(f, reduced_embeddings)"
      ],
      "metadata": {
        "id": "b0y0_WkO4iUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Train model and reduce dimensionality of embeddings\n",
        "umap_model = UMAP(n_components=2, n_neighbors=15, random_state=42, metric=\"cosine\", verbose=True)\n",
        "reduced_embeddings_2d = umap_model.fit_transform(embeddings)\n",
        "\n",
        "with open(f'{base_path}/umap_2d_embeddings.npy', 'wb') as f:\n",
        "    np.save(f, reduced_embeddings_2d)"
      ],
      "metadata": {
        "id": "7F3P2w-u4jIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling"
      ],
      "metadata": {
        "id": "HkluBsTQ7p9T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "vX-8m2m77tUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def get_data(data_path, typ= 'json'):\n",
        "  if typ == 'txt':\n",
        "    with open(data_path, 'r',encoding=\"utf-8\") as f:\n",
        "      data = f.read().replace('\\n', '').rstrip()\n",
        "    f.close()\n",
        "    all_text = data.split('.')\n",
        "    all_chunks = create_chunks(all_text)\n",
        "  else:\n",
        "    with open(data_path, 'r') as f:\n",
        "      data = json.load(f)\n",
        "    f.close()\n",
        "    all_text = []\n",
        "    if 'text' not in data:\n",
        "      for user, user_data in data.items():\n",
        "        text = user_data['text']\n",
        "        all_text.extend(text)\n",
        "      all_text = ' '.join(all_text).split('.')\n",
        "      all_chunks = create_chunks(all_text)\n",
        "    else:\n",
        "      all_chunks = data['text']\n",
        "\n",
        "  return all_chunks"
      ],
      "metadata": {
        "id": "fuw0vj1x7u9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "from adjustText import adjust_text\n",
        "import matplotlib.patheffects as pe\n",
        "import textwrap\n",
        "\n",
        "def advanced_visualization(topic_model, reduced_embeddings, data):\n",
        "\n",
        "  # Define colors for the visualization to iterate over\n",
        "  colors = itertools.cycle(['#e6194b', '#3cb44b', '#ffe119', '#4363d8', '#f58231', '#911eb4', '#46f0f0', '#f032e6', '#bcf60c', '#fabebe', '#008080', '#e6beff', '#9a6324', '#fffac8', '#800000', '#aaffc3', '#808000', '#ffd8b1', '#000075', '#808080', '#ffffff', '#000000'])\n",
        "  color_key = {str(topic): next(colors) for topic in set(topic_model.topics_) if topic != -1}\n",
        "\n",
        "  # Prepare dataframe and ignore outliers\n",
        "  df = pd.DataFrame({\"x\": reduced_embeddings[:, 0], \"y\": reduced_embeddings[:, 1], \"Topic\": [str(t) for t in topic_model.topics_]})\n",
        "  df[\"Length\"] = [len(doc) for doc in data]\n",
        "  df = df.loc[df.Topic != \"-1\"]\n",
        "  df = df.loc[(df.y > -10) & (df.y < 10) & (df.x < 10) & (df.x > -10), :]\n",
        "  df[\"Topic\"] = df[\"Topic\"].astype(\"category\")\n",
        "\n",
        "  # Get centroids of clusters\n",
        "  mean_df = df.groupby(\"Topic\").mean().reset_index()\n",
        "  mean_df.Topic = mean_df.Topic.astype(int)\n",
        "  mean_df = mean_df.sort_values(\"Topic\")\n",
        "\n",
        "  fig = plt.figure(figsize=(20, 15))\n",
        "  sns.scatterplot(data=df, x='x', y='y', c=df['Topic'].map(color_key), alpha=0.4, sizes=(0.6, 10), size=\"Length\")\n",
        "\n",
        "  # Annotate top 50 topics\n",
        "  texts, xs, ys = [], [], []\n",
        "  for row in mean_df.iterrows():\n",
        "    topic = row[1][\"Topic\"]\n",
        "    name = textwrap.fill(topic_model.custom_labels_[int(topic)], 20)\n",
        "\n",
        "    if int(topic) <= 50:\n",
        "      xs.append(row[1][\"x\"])\n",
        "      ys.append(row[1][\"y\"])\n",
        "      texts.append(plt.text(row[1][\"x\"], row[1][\"y\"], name, size=10, ha=\"center\", color=color_key[str(int(topic))],\n",
        "                            path_effects=[pe.withStroke(linewidth=0.5, foreground=\"black\")]\n",
        "                            ))\n",
        "\n",
        "  # Adjust annotations such that they do not overlap\n",
        "  adjust_text(texts, x=xs, y=ys, time_lim=1, force_text=(0.01, 0.02), force_static=(0.01, 0.02), force_pull=(0.5, 0.5))\n",
        "  plt.axis('off')\n",
        "  plt.legend('', frameon=False)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "9IxN_J--7voX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ATTENTION !!!!\n",
        "\n",
        "\n",
        "## Run this cell only if you want to label topic using Large Language Model Llama.\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, pipeline\n",
        "\n",
        "# System prompt describes information given to all conversations\n",
        "system_prompt = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "You are a helpful, respectful and honest assistant for labeling topics.\n",
        "<</SYS>>\n",
        "\"\"\"\n",
        "# Example prompt demonstrating the output we are looking for\n",
        "example_prompt = \"\"\"\n",
        "I have a topic that contains the following documents:\n",
        "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
        "- Meat, but especially beef, is the word food in terms of emissions.\n",
        "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
        "\n",
        "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "\n",
        "[/INST] Environmental impacts of eating meat\n",
        "\"\"\"\n",
        "\n",
        "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
        "main_prompt = \"\"\"\n",
        "[INST]\n",
        "I have a topic that contains the following documents:\n",
        "[DOCUMENTS]\n",
        "\n",
        "The topic is described by the following keywords: '[KEYWORDS]'.\n",
        "\n",
        "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
        "[/INST]\n",
        "\"\"\"\n",
        "prompt = system_prompt + example_prompt + main_prompt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Llama-2-13B-chat-GPTQ\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"TheBloke/Llama-2-13B-chat-GPTQ\", device_map='auto')\n",
        "\n",
        "pipe = pipeline(\n",
        "    task='text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    temperature=0.1,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.1\n",
        ")\n",
        "\n",
        "def get_labels(topic_model):\n",
        "  labels = []\n",
        "  for i in list(topic_model.get_topic_info()['Llama2']):\n",
        "    t = i[0].split('\\n')\n",
        "    if len(t)== 1:\n",
        "      labels.append(t[0])\n",
        "    else:\n",
        "      if t[0] != '':\n",
        "        labels.append(t[0])\n",
        "      else:\n",
        "        if t[1].startswith('Label:'):\n",
        "          p = t[1].split('Label:')[1]\n",
        "          labels.append(p)\n",
        "        else:\n",
        "          labels.append(t[1])\n",
        "  return labels"
      ],
      "metadata": {
        "id": "u2wpt6Bg8ta8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from bertopic import BERTopic\n",
        "from bertopic.cluster import BaseCluster\n",
        "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, PartOfSpeech, TextGeneration\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "class Dimensionality:\n",
        "  \"\"\" Use this for pre-calculated reduced embeddings \"\"\"\n",
        "  def __init__(self, reduced_embeddings):\n",
        "    self.reduced_embeddings = reduced_embeddings\n",
        "\n",
        "  def fit(self, X):\n",
        "    return self\n",
        "\n",
        "  def transform(self, X):\n",
        "    return self.reduced_embeddings\n",
        "\n",
        "\n",
        "class TopicModelling:\n",
        "  def __init__(self, base_path, embedding_model='all-MiniLM-L6-v2'):\n",
        "    self.base_path = base_path\n",
        "    self.embedding_model_name = embedding_model\n",
        "    self.prepare_paths()\n",
        "\n",
        "  def save_model(self):\n",
        "      save_path = os.path.join(self.base_path, 'final')\n",
        "      self.topic_model.save(save_path, serialization=\"safetensors\",\n",
        "                            save_ctfidf=True, save_embedding_model=self.embedding_model_name)\n",
        "\n",
        "\n",
        "  def load_model(self, model_name='final'):\n",
        "    self.topic_model = BERTopic.load(os.path.join(self.base_path, model_name))\n",
        "    self.load_embeddings()\n",
        "    self.load_data()\n",
        "    print('Model Loaded Successfully')\n",
        "    return self.topic_model\n",
        "\n",
        "  def prepare_paths(self):\n",
        "    self.data_path = os.path.join(self.base_path, 'only_text.json')\n",
        "    self.embeddings_path = os.path.join(self.base_path, 'embeddings.npy')\n",
        "    self.umap_embeddings_path = os.path.join(self.base_path, 'umap_embeddings.npy')\n",
        "    self.umap_embeddings_path_2d = os.path.join(self.base_path, 'umap_2d_embeddings.npy')\n",
        "    self.vocab_path = os.path.join(self.base_path, 'vocab.txt')\n",
        "\n",
        "  def load_data(self):\n",
        "    print('Fetching data....')\n",
        "    self.data = get_data(self.data_path)\n",
        "\n",
        "  def load_embeddings(self):\n",
        "    print('Fetching Embeddings...')\n",
        "    self.embeddings = np.load(self.embeddings_path)\n",
        "    self.reduced_embeddings = np.load(self.umap_embeddings_path)\n",
        "    self.reduced_embeddings_2d = np.load(self.umap_embeddings_path_2d)\n",
        "\n",
        "  def load_vocab(self):\n",
        "    print('Fetching vocab...')\n",
        "    with open (self.vocab_path, 'rb') as fp:\n",
        "        self.vocab = pickle.load(fp)\n",
        "\n",
        "  def load_models(self):\n",
        "    print('Loading Models...')\n",
        "    self.embedding_model = SentenceTransformer(self.embedding_model_name)\n",
        "    self.umap_model = Dimensionality(self.reduced_embeddings)\n",
        "    self.hdbscan_model = BaseCluster()\n",
        "    # Find clusters of semantically similar documents\n",
        "    hdbscan_model = HDBSCAN(min_samples=30, gen_min_span_tree=True, prediction_data=False, min_cluster_size=30, verbose=True)\n",
        "    self.clusters = hdbscan_model.fit(self.reduced_embeddings).labels_\n",
        "    sw = stopwords.words()\n",
        "    self.vectorizer_model = CountVectorizer(vocabulary=self.vocab, stop_words=sw)\n",
        "    keybert_model = KeyBERTInspired()\n",
        "\n",
        "    # Part-of-Speech\n",
        "    pos_model = PartOfSpeech(\"en_core_web_sm\")\n",
        "\n",
        "    # MMR\n",
        "    mmr_model = MaximalMarginalRelevance(diversity=0.3)\n",
        "\n",
        "    #Uncomment following line for using llama model for labelling\n",
        "    #llama2 = TextGeneration(pipe, prompt=prompt)\n",
        "\n",
        "    # All representation models\n",
        "    self.representation_model = {\n",
        "        \"KeyBERT\": keybert_model,\n",
        "        # \"OpenAI\": openai_model,  # Uncomment if you will use OpenAI\n",
        "        \"MMR\": mmr_model,\n",
        "        \"POS\": pos_model,\n",
        "        # \"Llama2\": llama2, # Uncomment for using Llama\n",
        "    }\n",
        "\n",
        "  def get_topic_model(self):\n",
        "    self.load_data()\n",
        "    self.load_embeddings()\n",
        "    self.load_vocab()\n",
        "    self.load_models()\n",
        "    print('Modelling...')\n",
        "    self.topic_model= BERTopic(\n",
        "            embedding_model=self.embedding_model,\n",
        "            umap_model=self.umap_model,\n",
        "            hdbscan_model=self.hdbscan_model,\n",
        "            vectorizer_model=self.vectorizer_model,\n",
        "            representation_model=self.representation_model,\n",
        "            verbose=True\n",
        "    ).fit(self.data, embeddings=self.embeddings, y=self.clusters)\n",
        "\n",
        "    #Uncomment for using llama labels\n",
        "    #llama2_labels = [label[0][0].split(\"\\n\")[0] for label in self.topic_model.get_topics(full=True)[\"Llama2\"].values()]\n",
        "    #self.topic_model.set_topic_labels(llama2_labels)\n",
        "\n",
        "    return self.topic_model\n",
        "\n"
      ],
      "metadata": {
        "id": "Po0aHd4z78uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Modelling"
      ],
      "metadata": {
        "id": "7kAQKKlI9OWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "tp = TopicModelling(base_path=base_path)\n",
        "topic_model = tp.get_topic_model()\n",
        "\n",
        "topic_model.get_topic_info()\n",
        "\n",
        "# Check https://github.com/MaartenGr/BERTopic to use other functionalities BERTopic"
      ],
      "metadata": {
        "id": "QdGfyaVA8AnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optional"
      ],
      "metadata": {
        "id": "7MUSYpvK-BKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save Model to the base path\n",
        "tp.save_model()"
      ],
      "metadata": {
        "id": "FV6xZenF9cw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Saved Model\n",
        "tp = TopicModelling(base_path=base_path)\n",
        "topic_model = tp.load_model()"
      ],
      "metadata": {
        "id": "q5DqgA8x9mxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes labelled topics using llama are incorrect, using following code we can fix it\n",
        "llama2_labels = get_labels(topic_model)\n",
        "topic_model.set_topic_labels(llama2_labels)"
      ],
      "metadata": {
        "id": "nusKdoeT938t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization"
      ],
      "metadata": {
        "id": "KFANzEGL-QEB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_hierarchy(custom_labels=False)  # Make custom_labels=True if llama model was used"
      ],
      "metadata": {
        "id": "Z_IFZkkL-Erx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.visualize_documents(tp.data, custom_labels=False) # Make custom_labels=True if llama model was used"
      ],
      "metadata": {
        "id": "RZbT1Ik1-V7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# only if llama model was used for labels\n",
        "advanced_visualization(topic_model, tp.reduced_embeddings_2d, tp.data)"
      ],
      "metadata": {
        "id": "LDswoegV-TO6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}